# Databricks notebook source
# MAGIC %md-sandbox
# MAGIC # Detecting Sarcasm with Databricks
# MAGIC 
# MAGIC <div class="markdown-converter__text--rendered"><h3>Context</h3>
# MAGIC 
# MAGIC     <img src="https://memesbams.com/wp-content/uploads/2017/11/sheldon-sarcasm-meme.jpg" style="float:right; height: 450px; margin: 10px; padding: 10px"/>
# MAGIC 
# MAGIC <p>This dataset contains 1.3 million sarcastic comments from the internet commentary website **Reddit**. The dataset was generated by scraping comments from Reddit  containing the <code>\s</code> ( sarcasm) tag. This tag is often used by Redditors to indicate that their comment is in jest and not meant to be taken seriously, and is generally a reliable indicator of sarcastic content.</p>
# MAGIC   
# MAGIC <h3>Content</h3>
# MAGIC 
# MAGIC <p>The corpus has 1.3 million sarcastic statements and responses, as well as many non-sarcastic comments from the same source.</p>
# MAGIC 
# MAGIC <h3>Acknowledgements</h3>
# MAGIC 
# MAGIC <p>The data was gathered by: Mikhail Khodak and Nikunj Saunshi and Kiran Vodrahalli for their article "<a href="https://arxiv.org/abs/1704.05579" rel="nofollow">A Large Self-Annotated Corpus for Sarcasm</a>". The data is hosted <a href="http://nlp.cs.princeton.edu/SARC/0.0/" rel="nofollow">here</a>.</p>
# MAGIC 
# MAGIC <p>Citation:</p>
# MAGIC 
# MAGIC <pre><code>@unpublished{SARC,
# MAGIC   authors={Mikhail Khodak and Nikunj Saunshi and Kiran Vodrahalli},
# MAGIC   title={A Large Self-Annotated Corpus for Sarcasm},
# MAGIC   url={https://arxiv.org/abs/1704.05579},
# MAGIC   year=2017
# MAGIC }
# MAGIC </code></pre>
# MAGIC 
# MAGIC <h3>Inspiration</h3>
# MAGIC 
# MAGIC <ul>
# MAGIC <li>Predicting sarcasm and relevant NLP features (e.g. subjective determinant, racism, conditionals, sentiment heavy words, "Internet Slang" and specific phrases). </li>
# MAGIC <li>Sarcasm vs Sentiment</li>
# MAGIC <li>Unusual linguistic features such as caps, italics, or elongated words. e.g., "Yeahhh, I'm sure THAT is the right answer".</li>
# MAGIC <li>Topics that people tend to react to sarcastically</li>
# MAGIC </ul></div>
# MAGIC 
# MAGIC 
# MAGIC https://www.kaggle.com/danofer/sarcasm
# MAGIC 
# MAGIC <!--
# MAGIC # mm-demo
# MAGIC # demo-ready
# MAGIC # TF-IDF
# MAGIC -->

# COMMAND ----------

# MAGIC %md #### Explore data

# COMMAND ----------

trainBalancedSarcasmDF = spark.read.option("header", True).csv("/mnt/mikem/tmp/train-balanced-sarcasm.csv")
trainBalancedSarcasmDF.createOrReplaceTempView('data')

# COMMAND ----------

df = sql("""
  select cast(label as int), concat(parent_comment,"\n",comment) as comment 
  from data where comment is not null 
  and parent_comment is not null 
  limit 100000""")

display(df)

# COMMAND ----------

# MAGIC %md #### NLP Pipeline

# COMMAND ----------

from sparknlp.annotator import *
from sparknlp.common import *
from sparknlp.base import *

from pyspark.ml import Pipeline

document_assembler = DocumentAssembler() \
    .setInputCol("comment") \
    .setOutputCol("document")
    
sentence_detector = SentenceDetector() \
    .setInputCols(["document"]) \
    .setOutputCol("sentence") \
    .setUseAbbreviations(True)
    
tokenizer = Tokenizer() \
  .setInputCols(["sentence"]) \
  .setOutputCol("token")

stemmer = Stemmer() \
    .setInputCols(["token"]) \
    .setOutputCol("stem")
    
normalizer = Normalizer() \
    .setInputCols(["stem"]) \
    .setOutputCol("normalized")

finisher = Finisher() \
    .setInputCols(["normalized"]) \
    .setOutputCols(["ntokens"]) \
    .setOutputAsArray(True) \
    .setCleanAnnotations(True)

nlp_pipeline = Pipeline(stages=[document_assembler, sentence_detector, tokenizer, stemmer, normalizer, finisher])
nlp_model = nlp_pipeline.fit(df)

processed = nlp_model.transform(df)

# COMMAND ----------

display(processed.where("label = 1").select("comment"))

# COMMAND ----------

train, test = processed.randomSplit(weights=[0.7, 0.3], seed=123)
print(train.count())
print(test.count())

# COMMAND ----------

# MAGIC %md #### Train Classifier

# COMMAND ----------

from pyspark.ml import feature as spark_ft

stopWords = spark_ft.StopWordsRemover.loadDefaultStopWords('english')
sw_remover = spark_ft.StopWordsRemover(inputCol='ntokens', outputCol='clean_tokens', stopWords=stopWords)
tf = spark_ft.CountVectorizer(vocabSize=500, inputCol='clean_tokens', outputCol='tf')
idf = spark_ft.IDF(minDocFreq=5, inputCol='tf', outputCol='idf')

feature_pipeline = Pipeline(stages=[sw_remover, tf, idf])
feature_model = feature_pipeline.fit(train)

train_featurized = feature_model.transform(train).persist()

# COMMAND ----------

display(train_featurized.groupBy("label").count())

# COMMAND ----------

from pyspark.ml import classification as spark_cls

rf = spark_cls. RandomForestClassifier(labelCol="label", featuresCol="idf", numTrees=100)

model = rf.fit(train_featurized)

# COMMAND ----------

# MAGIC %md #### Predict

# COMMAND ----------

test_featurized = feature_model.transform(test)
preds = model.transform(test_featurized)

display(preds.select("comment", "label", "prediction"))

# COMMAND ----------

import pandas as pd
from sklearn import metrics as skmetrics

pred_pds = preds.select('comment', 'label', 'prediction').toPandas()

pd.DataFrame(
    data=skmetrics.confusion_matrix(pred_pds['label'], pred_pds['prediction']),
    columns=['pred ' + l for l in ['0','1']],
    index=['true ' + l for l in ['0','1']]
)

# COMMAND ----------

print(skmetrics.classification_report(pred_pds['label'], pred_pds['prediction'], target_names=['0','1']))